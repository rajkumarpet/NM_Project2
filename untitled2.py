# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fUyFcDwQ1vaWYzBUvhXBZLS6PKcyCd27

# Task
write a correct source code for this file

Here is all the data you need:
"card_transdata.csv.xlsx"

## Data loading

### Subtask:
Load the data from the provided Excel file "card_transdata.csv.xlsx" into a pandas DataFrame.

**Reasoning**:
Load the data from the excel file into a pandas DataFrame and display the first few rows and the shape of the DataFrame.
"""

import pandas as pd

try:
    df = pd.read_excel('card_transdata.csv.xlsx')
    display(df.head())
    print(df.shape)
except FileNotFoundError:
    print("Error: 'card_transdata.csv.xlsx' not found.")
except Exception as e:
    print(f"An error occurred: {e}")

"""## Data exploration

### Subtask:
Explore the loaded data to understand its characteristics.

**Reasoning**:
Explore the data by examining data types, missing values, descriptive statistics, distributions of categorical features, correlations between numerical features, and the target variable distribution.
"""

# Examine data types
print(df.info())

# Check for missing values
print(df.isnull().sum())
missing_percentage = (df.isnull().sum() / len(df)) * 100
print(missing_percentage)

# Descriptive statistics for numerical columns
print(df.describe())

# Analyze categorical features
for col in ['repeat_retailer', 'used_chip', 'used_pin_number', 'online_order', 'fraud']:
    print(f"Value counts for {col}:\n{df[col].value_counts()}\n")

# Correlation between numerical features
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(10, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

# Distribution of the target variable
print(df['fraud'].value_counts(normalize=True))
plt.figure(figsize=(6, 4))
sns.countplot(x='fraud', data=df)
plt.title('Distribution of Fraudulent Transactions')
plt.show()

"""## Data cleaning

### Subtask:
Clean the data by addressing potential outliers in numerical features and handling any inconsistencies.

**Reasoning**:
Identify and handle outliers in the specified numerical columns using box plots and winsorization, check data types, remove duplicates, and validate the data.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Outlier detection and treatment
numerical_cols = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price']
for col in numerical_cols:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=df[col])
    plt.title(f'Box Plot of {col}')
    plt.show()

    # Winsorization to handle outliers
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

# Data Type Consistency (already checked in previous step, no action needed)

# Duplicate Rows
duplicate_rows = df[df.duplicated()]
num_duplicates = len(duplicate_rows)
print(f"Number of duplicate rows: {num_duplicates}")
if num_duplicates > 0:
    df.drop_duplicates(inplace=True)
    print(f"Removed {num_duplicates} duplicate rows.")

# Data Validation
print(df.describe())
print(df.info())
print(df.shape)

for col in numerical_cols:
    plt.figure(figsize=(8, 6))
    sns.histplot(df[col])
    plt.title(f'Distribution of {col} after outlier handling')
    plt.show()

"""## Data preparation

### Subtask:
Prepare the data for modeling by scaling numerical features and encoding categorical features.

**Reasoning**:
Prepare the data for modeling by scaling numerical features and encoding categorical features.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler, OneHotEncoder
import pandas as pd
import numpy as np

# Define features (X) and target (y)
X = df.drop('fraud', axis=1)
y = df['fraud']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


# Identify numerical and categorical features
numerical_features = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price']
categorical_features = ['repeat_retailer', 'used_chip', 'used_pin_number', 'online_order']

# Scale numerical features using RobustScaler
scaler = RobustScaler()
X_train_numerical_scaled = scaler.fit_transform(X_train[numerical_features])
X_test_numerical_scaled = scaler.transform(X_test[numerical_features])

# Encode categorical features using OneHotEncoder
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
X_train_categorical_encoded = encoder.fit_transform(X_train[categorical_features])
X_test_categorical_encoded = encoder.transform(X_test[categorical_features])

# Create new DataFrames for scaled numerical and encoded categorical features
X_train_numerical_scaled_df = pd.DataFrame(X_train_numerical_scaled, columns=[f'{col}_scaled' for col in numerical_features], index=X_train.index)
X_test_numerical_scaled_df = pd.DataFrame(X_test_numerical_scaled, columns=[f'{col}_scaled' for col in numerical_features], index=X_test.index)

X_train_categorical_encoded_df = pd.DataFrame(X_train_categorical_encoded, columns=encoder.get_feature_names_out(categorical_features), index=X_train.index)
X_test_categorical_encoded_df = pd.DataFrame(X_test_categorical_encoded, columns=encoder.get_feature_names_out(categorical_features), index=X_test.index)

# Concatenate scaled numerical and encoded categorical features
X_train_prepared = pd.concat([X_train_numerical_scaled_df, X_train_categorical_encoded_df], axis=1)
X_test_prepared = pd.concat([X_test_numerical_scaled_df, X_test_categorical_encoded_df], axis=1)

# Verify the shape and data types of the prepared DataFrame
print(X_train_prepared.info())
print(X_test_prepared.info())
print(X_train_prepared.shape)
print(X_test_prepared.shape)
print(X_train_prepared.isnull().sum())
print(X_test_prepared.isnull().sum())
display(X_train_prepared.head())
display(X_test_prepared.head())

"""## Data analysis

### Subtask:
Perform exploratory data analysis on the prepared datasets to gain insights.

**Reasoning**:
Analyze the distribution of the target variable ('fraud') and explore the relationships between features and the target variable using visualizations and statistical measures. Calculate the class imbalance ratio.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Target variable distribution analysis
plt.figure(figsize=(8, 6))
sns.countplot(x='fraud', data=df)
plt.title('Distribution of Fraudulent Transactions in the Dataset')
plt.show()

fraud_counts = df['fraud'].value_counts()
class_imbalance_ratio = fraud_counts[0] / fraud_counts[1]
print(f"Class Imbalance Ratio (Non-Fraudulent/Fraudulent): {class_imbalance_ratio}")


# Relationship between numerical features and target variable
numerical_features = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price']

plt.figure(figsize=(15, 5))
for i, col in enumerate(numerical_features):
  plt.subplot(1, 3, i + 1)
  sns.boxplot(x='fraud', y=col, data=df)
  plt.title(f'Distribution of {col} by Fraudulent Transactions')
plt.tight_layout()
plt.show()

# Relationship between categorical features and target variable
categorical_features = ['repeat_retailer', 'used_chip', 'used_pin_number', 'online_order']
plt.figure(figsize=(16, 8))
for i, col in enumerate(categorical_features):
    plt.subplot(2, 2, i + 1)
    fraud_rates = df.groupby(col)['fraud'].mean()
    sns.barplot(x=fraud_rates.index, y=fraud_rates.values)
    plt.title(f'Average Fraud Rate by {col}')
    plt.ylabel('Fraud Rate')
plt.tight_layout()
plt.show()

# Correlation analysis
plt.figure(figsize=(12, 10))
correlation_matrix = pd.concat([X_train_prepared, y_train], axis=1).corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Features (Including Target)')
plt.show()

"""## Summary:

### Q&A

No questions were posed in the provided data analysis task.


### Data Analysis Key Findings

* **Data Loading:** Successfully loaded 1,000,000 rows and 8 columns from "card_transdata.csv.xlsx" into a pandas DataFrame (`df`).
* **Data Exploration:**  No missing values were found. Potential outliers observed in 'distance_from_home', 'distance_from_last_transaction', and 'ratio_to_median_purchase_price'. Significant class imbalance in the 'fraud' target variable (91.3% non-fraudulent transactions).
* **Data Cleaning:** 1127 duplicate rows were removed. Outliers in 'distance_from_home', 'distance_from_last_transaction', and 'ratio_to_median_purchase_price' were handled using Winsorization. The cleaned DataFrame (`df`) has 998873 rows and 8 columns.
* **Data Preparation:** Data was split into training (799098 rows) and testing (199775 rows) sets. Numerical features were scaled using `RobustScaler`, and categorical features were one-hot encoded using `OneHotEncoder`.  The prepared training and testing sets (`X_train_prepared`, `X_test_prepared`) each have 11 features.
* **Data Analysis:** Confirmed the class imbalance.  Relationships between numerical and categorical features and the target variable 'fraud' were visualized using boxplots and barplots, respectively. A correlation matrix, including the target variable, was also generated to help identify potential multicollinearity.


### Insights or Next Steps

* **Address Class Imbalance:** The significant class imbalance needs to be addressed before model training. Techniques like oversampling the minority class (fraudulent transactions), undersampling the majority class, or using cost-sensitive learning algorithms should be considered.
* **Feature Engineering:** Explore further feature engineering based on the relationships observed in the exploratory data analysis.  For example, create interaction terms between features that show high correlation with fraud or use domain expertise to create new features.

"""